{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import requests\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to train a model to determine whether a post is from the one of the two following subreddits. In order to do this we will need a large number fo samples from both. The API we are using is an opensource project, and in order to avoid overtaxing the server we have built a sleep time into our API request function. Those trying to replicate this prokject should take care to include some measure of courtesy when using the API. The host has been known to block abussive IP adresses. \n",
    "\n",
    "Facts - https://www.reddit.com/r/facts/\n",
    "FakeFacts - https://www.reddit.com/r/FakeFacts/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Url and pull paramters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to be using the Pushshift APi - https://github.com/pushshift/api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api setting\n",
    "url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create json file from request results\n",
    "# access the df through the data key \n",
    "# return df\n",
    "def res_to_df(res):\n",
    "    data = res.json()\n",
    "    posts = data['data']\n",
    "    return pd.DataFrame(posts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a sleep controlled function for pulling subreddit information from the API that take a number of iterations, a subreddit name, and a UTC time to set the the most recent post in the return dataframe. The function is built to pull 50 posts at a time, and to sleep for a period of time between each pull. Each request result is concatonated into a master_df and returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posts(strokes, subreddit, target_time):\n",
    "    df = pd.DataFrame()\n",
    "    master_df = pd.DataFrame()\n",
    "    \n",
    "   \n",
    "    for i in range(strokes):\n",
    "        try:\n",
    "            params = {\n",
    "                'subreddit': subreddit,\n",
    "                'size': 50,\n",
    "                'before': target_time}\n",
    "\n",
    "            res = requests.get(url,params)\n",
    "            data = res.json()\n",
    "            posts = data['data']\n",
    "            df = pd.DataFrame(posts)\n",
    "\n",
    "            frames = [df,master_df]\n",
    "            master_df = pd.concat(frames, axis= 0 , ignore_index = True)\n",
    "\n",
    "            target_time = df['created_utc'].min()\n",
    "\n",
    "            time.sleep(10)\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    \n",
    "# for when created_utc does not exist     \n",
    "\n",
    "       \n",
    "    return master_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run our API request function for both our Subreddit pages and save the resulting dataframes to a single master_df . We neeed to give it a UTC time for the most recent post that we want returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calling API request function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function needs run several times. Furst run this section, then run the merger section so that out results df updates with new API results. The UTC time code will need updated before each new request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_df = get_posts(20, 'facts', 1594746614)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiction_df = get_posts(20, 'FakeFacts', 1540795916)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results_df is our full output from both subreddit api requests. We will continue to concatonate new api request to to this df in untill we have a large enough sample size.\n",
    "\n",
    "On the first run you will need to uncomment the line of code imediately below for the first and only the first request df merger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment this for the first time running\n",
    "# results_df = pd.DataFrame()\n",
    "\n",
    "# update fiction \n",
    "results_df = pd.concat([results_df,fiction_df], axis = 0, ignore_index = True)\n",
    "\n",
    "# update tact\n",
    "results_df = pd.concat([results_df,fact_df], axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to maintian an even sample size from both sources we will continue to pull from both untill one returns fewer samples than the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 74)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(524, 81)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fiction_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the outputed UTC below codes to pull next batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fact_utc: 1587720304\n",
      "Fiction_utc: 1332353296\n"
     ]
    }
   ],
   "source": [
    "last_fact = fact_df['created_utc'].min()\n",
    "last_fic = fiction_df['created_utc'].min()\n",
    "\n",
    "print(f'Fact_utc: {last_fact}')\n",
    "print(f'Fiction_utc: {last_fic}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5524, 93)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Fake Facts Subreddit ran out of posts to scrape after  2,524 observations. We will have slightly uneveen classes but this is ok for now. We can asses and change with in EDA if we find we need to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clip & Clean Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe will narrow down our features to just a text and word character length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_df = results_df[['subreddit','selftext','title',]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Removed '[removed]' from selftext column\n",
    "\n",
    "2) Created Has_selftext feature\n",
    "\n",
    "3) Combine selftext and title fields\n",
    "\n",
    "4) Create feature for Characeter lenght \n",
    "\n",
    "Most of our sibureddits selftexts are hyperlinks and. so thier pressence or abssence wil likely be a predicitve feature when trying to determine if our post cam from the facts, or FakeFacts reddit page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_df.selftext = clip_df.selftext.replace(['[removed]'],' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add title and self text\n",
    "clip_df['selftext'] = clip_df['title'] + ' ' + clip_df['selftext']\n",
    "\n",
    "# fill hold over columns that had NAns with title\n",
    "clip_df['selftext'] = clip_df['selftext'].fillna(clip_df['title'])\n",
    "\n",
    "# drop title \n",
    "clip_df.drop(columns = 'title', inplace = True)\n",
    "\n",
    "# make column for length of selftext\n",
    "clip_df['selftext_length'] = [len(post) for post in clip_df.selftext]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "We should have a df with columns for our subreddit, the text, and the lenght of the text in characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selftext_length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FakeFacts</th>\n",
       "      <td>176.398177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>facts</th>\n",
       "      <td>187.203667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           selftext_length\n",
       "subreddit                 \n",
       "FakeFacts       176.398177\n",
       "facts           187.203667"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>selftext</th>\n",
       "      <th>selftext_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FakeFacts</td>\n",
       "      <td>The origin of the word \"yeet\" is in old englis...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FakeFacts</td>\n",
       "      <td>Did you know LEGO was named after the phrase “...</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FakeFacts</td>\n",
       "      <td>Taking the crust off pizza is considered disre...</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FakeFacts</td>\n",
       "      <td>The Catholic church has a secret bible that al...</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FakeFacts</td>\n",
       "      <td>The earth is round</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit                                           selftext  \\\n",
       "0  FakeFacts  The origin of the word \"yeet\" is in old englis...   \n",
       "1  FakeFacts  Did you know LEGO was named after the phrase “...   \n",
       "2  FakeFacts  Taking the crust off pizza is considered disre...   \n",
       "3  FakeFacts  The Catholic church has a secret bible that al...   \n",
       "4  FakeFacts                               The earth is round     \n",
       "\n",
       "   selftext_length  \n",
       "0               50  \n",
       "1              117  \n",
       "2               65  \n",
       "3               99  \n",
       "4               20  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(clip_df.groupby(['subreddit']).mean(),clip_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5524, 3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we export as a csv and pickle our datframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('./data/raw_reddit.csv',  index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('raw.pkl','wb') as raw_pickle:\n",
    "    pickle.dump(clip_df, raw_pickle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
