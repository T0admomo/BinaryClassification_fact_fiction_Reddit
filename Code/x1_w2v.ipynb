{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FORGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to do initial model testingon two selected models. K-Nearest Neighbors and Support Vector Machines.\n",
    "\n",
    "We will fit and tune hyperparameters for those models in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim\n",
    "\n",
    "\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in corpus.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpus.pkl','rb') as clean_pickle:\n",
    "    corpus = pickle.load(clean_pickle)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"glove.6B.50d.txt\", \"rb\") as lines:\n",
    "    w2v = {line.split()[0]: np.array(map(float, line.split()[1:]))\n",
    "           for line in lines}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_corpus(corpus,column):\n",
    "    sparse_df = corpus[column]\n",
    "    \n",
    "    my_stop_words = ['https','com','www','people','know','actually',\n",
    "                     'world','time','years','fact','facts','fake','like',\n",
    "                     'sk','10','en','day','water','did','just']\n",
    "    \n",
    "    stop_words = text.ENGLISH_STOP_WORDS.union(my_stop_words)\n",
    "   \n",
    "    tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "    cvec = CountVectorizer(stop_words=stop_words)\n",
    "    sparse_df = cvec.fit_transform(sparse_df)\n",
    "\n",
    "    \n",
    "    sparse_df = pd.DataFrame(sparse_df.todense(), columns = cvec.get_feature_names())\n",
    "    sparse_df =  sparse_df.loc[(sparse_df.sum(axis=1) > 5), (sparse_df.sum(axis=0) >5)]\n",
    "    return sparse_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_df = prep_corpus(corpus,'selftext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HUD function for viewing summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diplays preview of dataframe for checking changes\n",
    "def disp_hud(hud):\n",
    "    base_group = corpus.groupby(['fact']).mean()\n",
    "    head = corpus.head(2)\n",
    "    \n",
    "    hud = [base_group,head]\n",
    "    disp = ['mean','preview']\n",
    "    \n",
    "    for i,li in enumerate(hud):\n",
    "        print(disp[i])\n",
    "        display(li)\n",
    "        \n",
    "#disp_hud(hud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        if len(word2vec)>0:\n",
    "            self.dim=len(word2vec[next(iter(glove_small))])\n",
    "        else:\n",
    "            self.dim=0\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] \n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_tokens(corpus,column):    \n",
    "    tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "    corpus[column] = [tokenizer.tokenize(row) for row in corpus[column]]\n",
    "    return corpus\n",
    "\n",
    "corpus = _make_tokens(corpus,'selftext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus['selftext'] = [word_tokenize(word) for word in corpus.selftext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            [The, origin, word, \"yeet\", old, english, .]\n",
       "1       [Did, LEGO, named, phrase, “let’s, ”?, In, 187...\n",
       "2       [Taking, crust, pizza, considered, disrespectf...\n",
       "3       [The, Catholic, church, secret, bible, allows,...\n",
       "4                                     [The, earth, round]\n",
       "                              ...                        \n",
       "5519    [Some, species, fish, naturally, change, sex, ...\n",
       "5520    [Believe, Not, :, Things, need, Justo, Smoker,...\n",
       "5521    [Heart, Attacks, -, How, avoid, live, healthie...\n",
       "5522    [The, Indonesian, Psychiatrists, Association, ...\n",
       "5523    [The, analog, computer, Antikythera, mechanism...\n",
       "Name: selftext, Length: 5524, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.selftext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = corpus['selftext']\n",
    "y = corpus.subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
    "                                                   stratify = y ,\n",
    "                                                   test_size = .33,\n",
    "                                                   random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(X)\n",
    "w2v = dict(zip(model.wv.index_to_key, model.wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading glove files, this may take a while\n",
    "# we're reading line by line and only saving vectors\n",
    "# that correspond to words from our training set\n",
    "# if you wan't to play around with the vectors and have \n",
    "# enough RAM - remove the 'if' line and load everything\n",
    "\n",
    "import struct \n",
    "\n",
    "GLOVE_6B_300D_PATH = \"glove.6B.300d.txt\"\n",
    "GLOVE_6B_50D_PATH = \"glove.6B.50d.txt\"\n",
    "encoding=\"utf-8\"\n",
    "\n",
    "\n",
    "glove_small = {}\n",
    "all_words = set(w for words in X for w in words)\n",
    "with open(GLOVE_6B_50D_PATH, \"rb\") as infile:\n",
    "    for line in infile:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode(encoding)\n",
    "        if (word in all_words):\n",
    "            nums=np.array(parts[1:], dtype=np.float32)\n",
    "            glove_small[word] = nums\n",
    "\n",
    "            \n",
    "glove_big = {}\n",
    "with open(GLOVE_6B_300D_PATH, \"rb\") as infile:\n",
    "    for line in infile:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode(encoding)\n",
    "        if word in all_words:\n",
    "            nums=np.array(parts[1:], dtype=np.float32)\n",
    "            glove_big[word] = nums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GO 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_1 = Pipeline([\n",
    "    ('word2vec', MeanEmbeddingVectorizer(w2v)),\n",
    "    ('support_vectors',LinearSVC(max_iter = 5_000, verbose = 1))\n",
    "]);\n",
    "\n",
    "nb_1 = Pipeline([\n",
    "    ('word2vec', MeanEmbeddingVectorizer(w2v)),\n",
    "    ('naive',BernoulliNB())\n",
    "]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('word2vec',\n",
       "                 <__main__.MeanEmbeddingVectorizer object at 0x000001E0C2EC2640>),\n",
       "                ('naive', BernoulliNB())])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_1.fit(X_train,y_train)\n",
    "nb_1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Train:0.5190487653645548 \n",
      "NB Test:0.5118064127653168\n",
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]svc Train:0.6195583032425137 \n",
      "svc Test:0.592426614481409\n"
     ]
    }
   ],
   "source": [
    "train = cross_val_score(nb_1,X_train,y_train)\n",
    "test = cross_val_score(nb_1,X_test,y_test)\n",
    "\n",
    "print(f'''NB Train:{train.mean()} \n",
    "NB Test:{test.mean()}''')\n",
    "\n",
    "train = cross_val_score(svc_1,X_train,y_train)\n",
    "test = cross_val_score(svc_1,X_test,y_test)\n",
    "\n",
    "print(f'''svc Train:{train.mean()} \n",
    "svc Test:{test.mean()}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_1 = Pipeline([\n",
    "    ('word2vec', MeanEmbeddingVectorizer(w2v)),\n",
    "    ('support_vectors',LinearSVC(max_iter = 5_000))\n",
    "]);\n",
    "\n",
    "nb_1 = Pipeline([\n",
    "    ('word2vec', MeanEmbeddingVectorizer(w2v)),\n",
    "    ('naive',BernoulliNB())\n",
    "]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Train:0.5190487653645548 \n",
      "NB Test:0.5118064127653168\n",
      "svc Train:0.6195583032425137 \n",
      "svc Test:0.592426614481409\n"
     ]
    }
   ],
   "source": [
    "train = cross_val_score(nb_1,X_train,y_train)\n",
    "test = cross_val_score(nb_1,X_test,y_test)\n",
    "\n",
    "print(f'''NB Train:{train.mean()} \n",
    "NB Test:{test.mean()}''')\n",
    "\n",
    "train = cross_val_score(svc_1,X_train,y_train)\n",
    "test = cross_val_score(svc_1,X_test,y_test)\n",
    "\n",
    "print(f'''svc Train:{train.mean()} \n",
    "svc Test:{test.mean()}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_svm_cv = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('scv',LinearSVC()),\n",
    "]);\n",
    "\n",
    "pipe_svm_tf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('scv',LinearSVC()),\n",
    "]);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SVC' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-c23a7210e5e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msvc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcvec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmnb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SVC' is not defined"
     ]
    }
   ],
   "source": [
    "svc = SVC()\n",
    "cvec = CountVectorizer()\n",
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
