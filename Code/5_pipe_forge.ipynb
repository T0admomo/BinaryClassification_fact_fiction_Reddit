{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FORGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to do initial model testingon two selected models. K-Nearest Neighbors and Support Vector Machines.\n",
    "\n",
    "We will fit and tune hyperparameters for those models in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, mean_squared_error\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim\n",
    "\n",
    "\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpus.pkl','rb') as clean_pickle:\n",
    "    corpus = pickle.load(clean_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.to_csv('./corpus.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = ['https','com','www','people','know','actually',\n",
    "                     'world','time','years','fact','facts','fake','like',\n",
    "                     'sk','10','en','day','water','did','just','the']\n",
    "    \n",
    "    # append custom stopwords to text.ENGLIS_STOP_WORDS\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(my_stop_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = corpus['selftext']\n",
    "y = corpus.subreddit\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
    "                                                   stratify = y ,\n",
    "                                                   test_size = .33,\n",
    "                                                   random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "Fitting 5 folds for each of 4000 candidates, totalling 20000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  29 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=6)]: Done 150 tasks      | elapsed:   38.8s\n",
      "[Parallel(n_jobs=6)]: Done 353 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=6)]: Done 847 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=6)]: Done 1576 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=6)]: Done 2280 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=6)]: Done 3148 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=6)]: Done 4230 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=6)]: Done 5410 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=6)]: Done 6535 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=6)]: Done 8165 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=6)]: Done 9660 tasks      | elapsed:  9.3min\n",
      "[Parallel(n_jobs=6)]: Done 11242 tasks      | elapsed: 11.0min\n",
      "[Parallel(n_jobs=6)]: Done 13014 tasks      | elapsed: 12.8min\n",
      "[Parallel(n_jobs=6)]: Done 14938 tasks      | elapsed: 14.6min\n",
      "[Parallel(n_jobs=6)]: Done 17082 tasks      | elapsed: 16.4min\n",
      "[Parallel(n_jobs=6)]: Done 19354 tasks      | elapsed: 18.2min\n",
      "[Parallel(n_jobs=6)]: Done 19989 out of 20000 | elapsed: 18.4min remaining:    0.5s\n",
      "[Parallel(n_jobs=6)]: Done 20000 out of 20000 | elapsed: 18.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('cvec',\n",
       "                                        CountVectorizer(stop_words=frozenset({'10',\n",
       "                                                                              'a',\n",
       "                                                                              'about',\n",
       "                                                                              'above',\n",
       "                                                                              'across',\n",
       "                                                                              'actually',\n",
       "                                                                              'after',\n",
       "                                                                              'afterwards',\n",
       "                                                                              'again',\n",
       "                                                                              'against',\n",
       "                                                                              'all',\n",
       "                                                                              'almost',\n",
       "                                                                              'alone',\n",
       "                                                                              'along',\n",
       "                                                                              'already',\n",
       "                                                                              'also',\n",
       "                                                                              'although',\n",
       "                                                                              'always',\n",
       "                                                                              'am',\n",
       "                                                                              'among',\n",
       "                                                                              'amongst',\n",
       "                                                                              'amoungst',\n",
       "                                                                              'amount',\n",
       "                                                                              'an',\n",
       "                                                                              'and',\n",
       "                                                                              'another',\n",
       "                                                                              'any',\n",
       "                                                                              'anyhow',\n",
       "                                                                              'anyone',\n",
       "                                                                              'anything', ...}))),\n",
       "                                       (...\n",
       "             param_grid={'cvec__max_df': array([0.2       , 0.21111111, 0.22222222, 0.23333333, 0.24444444,\n",
       "       0.25555556, 0.26666667, 0.27777778, 0.28888889, 0.3       ]),\n",
       "                         'cvec__min_df': array([0.001, 0.112, 0.223, 0.334, 0.445, 0.556, 0.667, 0.778, 0.889,\n",
       "       1.   ]),\n",
       "                         'logreg__C': array([0.001, 0.112, 0.223, 0.334, 0.445, 0.556, 0.667, 0.778, 0.889,\n",
       "       1.   ]),\n",
       "                         'logreg__dual': [True, False],\n",
       "                         'logreg__max_iter': [5000],\n",
       "                         'logreg__penalty': ['l1', 'l2']},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = Pipeline([\n",
    "        ('cvec', CountVectorizer(stop_words=stop_words)),\n",
    "        ('logreg',LogisticRegression(solver = 'saga'))]);\n",
    "\n",
    "\n",
    "\n",
    "logreg_params = {\n",
    "    'cvec__max_df': np.linspace(0.20,0.30,10),\n",
    "    'cvec__min_df': np.linspace(0.001,1,10),\n",
    "    'logreg__penalty': ['l1','l2'],\n",
    "    'logreg__dual': [True,False],\n",
    "    'logreg__C': np.linspace(0.001,1,10),\n",
    "    #'logreg__solver': ['newton-cg','lbfgs','liblinear','sag','saga'],\n",
    "    'logreg__max_iter': [5_000]\n",
    "}    \n",
    "\n",
    "\n",
    "grid = GridSearchCV(logreg, logreg_params, cv=5, n_jobs = 6, verbose = 2)\n",
    "\n",
    "%time\n",
    "grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cvec',\n",
       "                 CountVectorizer(max_df=0.22222222222222224, min_df=0.001,\n",
       "                                 stop_words=frozenset({'10', 'a', 'about',\n",
       "                                                       'above', 'across',\n",
       "                                                       'actually', 'after',\n",
       "                                                       'afterwards', 'again',\n",
       "                                                       'against', 'all',\n",
       "                                                       'almost', 'alone',\n",
       "                                                       'along', 'already',\n",
       "                                                       'also', 'although',\n",
       "                                                       'always', 'am', 'among',\n",
       "                                                       'amongst', 'amoungst',\n",
       "                                                       'amount', 'an', 'and',\n",
       "                                                       'another', 'any',\n",
       "                                                       'anyhow', 'anyone',\n",
       "                                                       'anything', ...}))),\n",
       "                ('logreg',\n",
       "                 LogisticRegression(C=0.223, max_iter=5000, solver='saga'))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7330433672538936"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_preds = grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = classification_report(y_test,log_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   FakeFacts       0.74      0.73      0.73       833\n",
      "       facts       0.77      0.78      0.78       990\n",
      "\n",
      "    accuracy                           0.76      1823\n",
      "   macro avg       0.76      0.75      0.75      1823\n",
      "weighted avg       0.76      0.76      0.76      1823\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7410861217772902"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(y_test,grid.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _chart_grid(grid,name):\n",
    "    temp_df = pd.DataFrame()\n",
    "    probs = grid.predict_proba(X_test)\n",
    "    preds = grid.predict(X_test)\n",
    "    \n",
    "    temp_df[f'{name} Probability'] = [prob[1] for prob in probs]\n",
    "    temp_df[f'{name} Prediction'] = [pred for pred in log_preds]\n",
    "    temp_df[f'{name} Binary'] = np.where(temp_df[f'{name} Prediction'] == 'facts',1,0)\n",
    "    \n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df =  _chart_grid(grid,'LogReg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.where(y_test == 'facts',1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predictions_df['LogReg Binary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25891387822270984"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preds = grid.predict(X_test)\n",
    "mean_squared_error(y_test,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LogReg Probability</th>\n",
       "      <th>LogReg Prediction</th>\n",
       "      <th>LogReg Binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.871724</td>\n",
       "      <td>facts</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.636360</td>\n",
       "      <td>facts</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.997663</td>\n",
       "      <td>facts</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.277183</td>\n",
       "      <td>FakeFacts</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.650310</td>\n",
       "      <td>facts</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>0.999900</td>\n",
       "      <td>facts</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1819</th>\n",
       "      <td>0.280876</td>\n",
       "      <td>FakeFacts</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820</th>\n",
       "      <td>0.742594</td>\n",
       "      <td>facts</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1821</th>\n",
       "      <td>0.627395</td>\n",
       "      <td>facts</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822</th>\n",
       "      <td>0.436111</td>\n",
       "      <td>FakeFacts</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1823 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      LogReg Probability LogReg Prediction  LogReg Binary\n",
       "0               0.871724             facts              1\n",
       "1               0.636360             facts              1\n",
       "2               0.997663             facts              1\n",
       "3               0.277183         FakeFacts              0\n",
       "4               0.650310             facts              1\n",
       "...                  ...               ...            ...\n",
       "1818            0.999900             facts              1\n",
       "1819            0.280876         FakeFacts              0\n",
       "1820            0.742594             facts              1\n",
       "1821            0.627395             facts              1\n",
       "1822            0.436111         FakeFacts              0\n",
       "\n",
       "[1823 rows x 3 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'svm__C': 0.112,\n",
    " 'svm__max_iter': 1000,\n",
    " 'tfidf__max_df': 0.223,\n",
    " 'tfidf__min_df': 0.001,\n",
    " 'tfidf__ngram_range': (1, 3),\n",
    " 'tfidf__norm': 'l2'}\n",
    "\n",
    "{'svm__C': 0.556,\n",
    " 'svm__max_iter': 1000,\n",
    " 'tfidf__max_df': 0.2,\n",
    " 'tfidf__min_df': 0.0001,\n",
    " 'tfidf__ngram_range': (1, 3),\n",
    " 'tfidf__norm': 'l2'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "Fitting 5 folds for each of 18000 candidates, totalling 90000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  29 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=6)]: Done 150 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=6)]: Done 353 tasks      | elapsed:   22.8s\n",
      "[Parallel(n_jobs=6)]: Done 636 tasks      | elapsed:   38.6s\n",
      "[Parallel(n_jobs=6)]: Done 1001 tasks      | elapsed:   58.8s\n",
      "[Parallel(n_jobs=6)]: Done 1446 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=6)]: Done 1973 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=6)]: Done 2580 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=6)]: Done 3269 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=6)]: Done 4038 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=6)]: Done 4889 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=6)]: Done 5820 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=6)]: Done 6833 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=6)]: Done 7926 tasks      | elapsed:  7.6min\n",
      "[Parallel(n_jobs=6)]: Done 9101 tasks      | elapsed:  8.7min\n",
      "[Parallel(n_jobs=6)]: Done 10356 tasks      | elapsed:  9.9min\n",
      "[Parallel(n_jobs=6)]: Done 11693 tasks      | elapsed: 11.2min\n",
      "[Parallel(n_jobs=6)]: Done 13110 tasks      | elapsed: 12.6min\n",
      "[Parallel(n_jobs=6)]: Done 14609 tasks      | elapsed: 14.0min\n",
      "[Parallel(n_jobs=6)]: Done 16188 tasks      | elapsed: 15.6min\n",
      "[Parallel(n_jobs=6)]: Done 17849 tasks      | elapsed: 17.2min\n",
      "[Parallel(n_jobs=6)]: Done 19590 tasks      | elapsed: 18.9min\n",
      "[Parallel(n_jobs=6)]: Done 21413 tasks      | elapsed: 20.6min\n",
      "[Parallel(n_jobs=6)]: Done 23316 tasks      | elapsed: 22.5min\n",
      "[Parallel(n_jobs=6)]: Done 25301 tasks      | elapsed: 24.4min\n",
      "[Parallel(n_jobs=6)]: Done 27366 tasks      | elapsed: 26.5min\n",
      "[Parallel(n_jobs=6)]: Done 29513 tasks      | elapsed: 28.5min\n",
      "[Parallel(n_jobs=6)]: Done 31740 tasks      | elapsed: 30.7min\n",
      "[Parallel(n_jobs=6)]: Done 34049 tasks      | elapsed: 32.9min\n",
      "[Parallel(n_jobs=6)]: Done 36438 tasks      | elapsed: 35.2min\n",
      "[Parallel(n_jobs=6)]: Done 38909 tasks      | elapsed: 37.6min\n",
      "[Parallel(n_jobs=6)]: Done 41460 tasks      | elapsed: 40.0min\n",
      "[Parallel(n_jobs=6)]: Done 44093 tasks      | elapsed: 42.5min\n",
      "[Parallel(n_jobs=6)]: Done 46806 tasks      | elapsed: 45.2min\n",
      "[Parallel(n_jobs=6)]: Done 49601 tasks      | elapsed: 47.8min\n",
      "[Parallel(n_jobs=6)]: Done 52476 tasks      | elapsed: 50.5min\n",
      "[Parallel(n_jobs=6)]: Done 55433 tasks      | elapsed: 53.3min\n",
      "[Parallel(n_jobs=6)]: Done 58470 tasks      | elapsed: 56.1min\n",
      "[Parallel(n_jobs=6)]: Done 61589 tasks      | elapsed: 59.0min\n",
      "[Parallel(n_jobs=6)]: Done 64788 tasks      | elapsed: 62.0min\n",
      "[Parallel(n_jobs=6)]: Done 68069 tasks      | elapsed: 65.0min\n",
      "[Parallel(n_jobs=6)]: Done 71430 tasks      | elapsed: 68.1min\n",
      "[Parallel(n_jobs=6)]: Done 74873 tasks      | elapsed: 71.4min\n",
      "[Parallel(n_jobs=6)]: Done 78396 tasks      | elapsed: 74.7min\n",
      "[Parallel(n_jobs=6)]: Done 82001 tasks      | elapsed: 78.0min\n",
      "[Parallel(n_jobs=6)]: Done 85686 tasks      | elapsed: 81.4min\n",
      "[Parallel(n_jobs=6)]: Done 89453 tasks      | elapsed: 85.0min\n",
      "[Parallel(n_jobs=6)]: Done 90000 out of 90000 | elapsed: 85.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tfidf',\n",
       "                                        TfidfVectorizer(stop_words=frozenset({'10',\n",
       "                                                                              'a',\n",
       "                                                                              'about',\n",
       "                                                                              'above',\n",
       "                                                                              'across',\n",
       "                                                                              'actually',\n",
       "                                                                              'after',\n",
       "                                                                              'afterwards',\n",
       "                                                                              'again',\n",
       "                                                                              'against',\n",
       "                                                                              'all',\n",
       "                                                                              'almost',\n",
       "                                                                              'alone',\n",
       "                                                                              'along',\n",
       "                                                                              'already',\n",
       "                                                                              'also',\n",
       "                                                                              'although',\n",
       "                                                                              'always',\n",
       "                                                                              'am',\n",
       "                                                                              'among',\n",
       "                                                                              'amongst',\n",
       "                                                                              'amoungst',\n",
       "                                                                              'amount',\n",
       "                                                                              'an',\n",
       "                                                                              'and',\n",
       "                                                                              'another',\n",
       "                                                                              'any',\n",
       "                                                                              'anyhow',\n",
       "                                                                              'anyone',\n",
       "                                                                              'anything', ...}))),...\n",
       "                         'svm__max_iter': [1000, 2000, 3000],\n",
       "                         'tfidf__max_df': array([0.2       , 0.21111111, 0.22222222, 0.23333333, 0.24444444,\n",
       "       0.25555556, 0.26666667, 0.27777778, 0.28888889, 0.3       ]),\n",
       "                         'tfidf__min_df': array([0.0001    , 0.00064444, 0.00118889, 0.00173333, 0.00227778,\n",
       "       0.00282222, 0.00336667, 0.00391111, 0.00445556, 0.005     ]),\n",
       "                         'tfidf__ngram_range': [(1, 2), (1, 3), (1, 4)],\n",
       "                         'tfidf__norm': ['l1', 'l2']},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_tf = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "        ('svm', LinearSVC() ) ]);\n",
    "\n",
    "svm_tf_params = {\n",
    "    'tfidf__max_df': np.linspace(0.20,0.30,10),\n",
    "    'tfidf__min_df': np.linspace(0.0001,0.005,10),\n",
    "    'tfidf__ngram_range': [(1,2),(1,3),(1,4)],\n",
    "    'tfidf__norm' : ['l1','l2'],\n",
    "    \n",
    "    #'svm__loss': ['hinge','squared_hinge'],\n",
    "    #'svm__dual': [True,False],\n",
    "    'svm__C': np.linspace(0.001,1,10),\n",
    "    'svm__max_iter': [1_000,2_000,3_000]\n",
    "}    \n",
    "    \n",
    "grid_2 = GridSearchCV(svm_tf, svm_tf_params, cv=5, n_jobs = 6, verbose = 2)\n",
    "    #grid = GridSearchCV(logreg, logreg_params, cv=5, n_jobs = 6, verbose = 2)\n",
    "%time\n",
    "\n",
    "grid_2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7430462851515484"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'svm__C': 0.556,\n",
       " 'svm__max_iter': 1000,\n",
       " 'tfidf__max_df': 0.2,\n",
       " 'tfidf__min_df': 0.0001,\n",
       " 'tfidf__ngram_range': (1, 3),\n",
       " 'tfidf__norm': 'l2'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "Fitting 5 folds for each of 6000 candidates, totalling 30000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  29 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=6)]: Done 432 tasks      | elapsed:    8.0s\n",
      "[Parallel(n_jobs=6)]: Done 1244 tasks      | elapsed:   17.2s\n",
      "[Parallel(n_jobs=6)]: Done 2376 tasks      | elapsed:   29.8s\n",
      "[Parallel(n_jobs=6)]: Done 3836 tasks      | elapsed:   46.6s\n",
      "[Parallel(n_jobs=6)]: Done 5616 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=6)]: Done 7724 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=6)]: Done 10152 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=6)]: Done 12908 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=6)]: Done 15984 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=6)]: Done 19388 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=6)]: Done 23112 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=6)]: Done 27164 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=6)]: Done 30000 out of 30000 | elapsed:  5.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tfidf',\n",
       "                                        TfidfVectorizer(stop_words=frozenset({'10',\n",
       "                                                                              'a',\n",
       "                                                                              'about',\n",
       "                                                                              'above',\n",
       "                                                                              'across',\n",
       "                                                                              'actually',\n",
       "                                                                              'after',\n",
       "                                                                              'afterwards',\n",
       "                                                                              'again',\n",
       "                                                                              'against',\n",
       "                                                                              'all',\n",
       "                                                                              'almost',\n",
       "                                                                              'alone',\n",
       "                                                                              'along',\n",
       "                                                                              'already',\n",
       "                                                                              'also',\n",
       "                                                                              'although',\n",
       "                                                                              'always',\n",
       "                                                                              'am',\n",
       "                                                                              'among',\n",
       "                                                                              'amongst',\n",
       "                                                                              'amoungst',\n",
       "                                                                              'amount',\n",
       "                                                                              'an',\n",
       "                                                                              'and',\n",
       "                                                                              'another',\n",
       "                                                                              'any',\n",
       "                                                                              'anyhow',\n",
       "                                                                              'anyone',\n",
       "                                                                              'anything', ...}))),...\n",
       "                         'tfidf__max_df': array([0.2       , 0.21111111, 0.22222222, 0.23333333, 0.24444444,\n",
       "       0.25555556, 0.26666667, 0.27777778, 0.28888889, 0.3       ]),\n",
       "                         'tfidf__min_df': array([0.001, 0.112, 0.223, 0.334, 0.445, 0.556, 0.667, 0.778, 0.889,\n",
       "       1.   ]),\n",
       "                         'tfidf__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
       "                         'tfidf__tokenizer': [None,\n",
       "                                              RegexpTokenizer(pattern='\\\\w+|\\\\$[\\\\d\\\\.]+|\\\\S+', gaps=False, discard_empty=True, flags=re.UNICODE|re.MULTILINE|re.DOTALL)]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "        ('MNB',MultinomialNB())]);\n",
    "\n",
    "\n",
    "\n",
    "logreg_params = {\n",
    "    'tfidf__max_df': np.linspace(0.20,0.30,10),\n",
    "    'tfidf__min_df': np.linspace(0.001,1,10),\n",
    "    'tfidf__tokenizer': [None,tokenizer],\n",
    "    'tfidf__ngram_range':[(1,1),(1,2),(2,2)],\n",
    "    'MNB__alpha': np.linspace(0.001,1,10)  \n",
    "}    \n",
    "\n",
    "\n",
    "grid_3 = GridSearchCV(logreg, logreg_params, cv=5, n_jobs = 6, verbose = 2)\n",
    "\n",
    "%time\n",
    "grid_3.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7281850676587518"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
