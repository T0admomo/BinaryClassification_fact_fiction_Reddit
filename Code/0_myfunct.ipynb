{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP classification Modeling Function Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a copy of all of the functions we constructed and use in this porject. What the documentation does not explain for each function, we hope to elaborate in here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Api Request's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/pushshift/api \n",
    "\n",
    "We used Pushift API which is run on a small pirvate server. As such we modified our API request make small recursive pulls spread out over time to avoi clogging the network or recieving a ban."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posts(strokes, subreddit, target_time):\n",
    "    df = pd.DataFrame()\n",
    "    master_df = pd.DataFrame()\n",
    "    \n",
    "   \n",
    "    for i in range(strokes):\n",
    "        try:\n",
    "            # set the parameters for \n",
    "            params = {\n",
    "                'subreddit': subreddit,\n",
    "                'size': 50,\n",
    "                'before': target_time}\n",
    "\n",
    "            res = requests.get(url,params)\n",
    "            data = res.json()\n",
    "            posts = data['data']\n",
    "            df = pd.DataFrame(posts)\n",
    "\n",
    "            frames = [df,master_df]\n",
    "            master_df = pd.concat(frames, axis= 0 , ignore_index = True)\n",
    "\n",
    "            target_time = df['created_utc'].min()\n",
    "\n",
    "            time.sleep(10)\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    \n",
    "# for when created_utc does not exist     \n",
    "\n",
    "       \n",
    "    return master_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates dataframe from an HTML request response\n",
    "def res_to_df(res):\n",
    "    ''' input: html request results  \n",
    "        output: dataframe\n",
    "    '''\n",
    "    data = res.json()\n",
    "    posts = data['data']\n",
    "    return pd.DataFrame(posts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disp_hud(hud):\n",
    "    \n",
    "    \"\"\"Useful Hud for checking our tokenization and datframe concatonations work well\"\"\"\n",
    "   \n",
    "    # here we create tables we want to display \n",
    "    base_group = corpus.groupby(['fact']).mean()\n",
    "    head = corpus.head(2)\n",
    "    \n",
    "    # here we list them and pair them with names\n",
    "    hud = [base_group,head]\n",
    "    disp = ['mean','preview']\n",
    "    \n",
    "    # iterate through list of tables \n",
    "    # print name and displauy table\n",
    "    for i,li in enumerate(hud):\n",
    "        print(disp[i])\n",
    "        display(li)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a function for plotting our own custom confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom confusion matrix plot\n",
    "\n",
    "def disp_cm(model,X,y):\n",
    "    \n",
    "    \"\"\"\n",
    "    model: model name\n",
    "    X: train or test features list\n",
    "    \"\"\"\n",
    "    preds = model.predict(X)\n",
    "   \n",
    "    cm = confusion_matrix(y, preds)\n",
    "    \n",
    "    fig  = ConfusionMatrixDisplay(cm).plot();\n",
    "    fig = plt.title(f'{model[1]} \\n MSE: {mean_squared_error(y ,preds)}')\n",
    "    fig = plt.xlabel((('Fiction'),('Fact')))\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a series of functions for tokeninzing and modifying our datafarame. Here you will find find three steps which are then combined into a fourth which calls the prior three. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1). Use Regex Tokenizer to remove punction and whitespace in tokenized output of textual input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text field of each column and remove pnctuation.\n",
    "\n",
    "def make_tokens(corpus,column):    \n",
    "    \n",
    "    '''\n",
    "    corpus: dataframe with field to tokenize\n",
    "    column: the name of the column you would like to tokenize\n",
    "    \n",
    "    output: dataframe with tokenized text field\n",
    "    '''\n",
    "    tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "    corpus[column] = [tokenizer.tokenize(row) for row in corpus[column]]\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2). Remove tokens from coprus with using text.ENGLISH_STOP_WORDS and custom stop word list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokens(corpus,column):    \n",
    "    '''\n",
    "    corpus: dataframe with tokenized text field\n",
    "    column : text column\n",
    "    \n",
    "    output: dataframe with stopword tokens removed\n",
    "    '''\n",
    "    \n",
    "    # custom stop word list\n",
    "    my_stop_words = ['https','com','www','people','know','actually',\n",
    "                     'world','time','years','fact','facts','fake','like',\n",
    "                     'sk','10','en','day','water','did','just','the']\n",
    "    \n",
    "    # append custom stopwords to text.ENGLIS_STOP_WORDS\n",
    "    stop_words = text.ENGLISH_STOP_WORDS.union(my_stop_words)\n",
    "    \n",
    "    # CLean rows of words in stop words list\n",
    "    for row in corpus[column]:\n",
    "        for word in row:\n",
    "            if word in stop_words:\n",
    "                row.remove(word)\n",
    "    \n",
    "    # returns clean datframe.\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the circumstance you may want to perform make_tokens & clean_tokens without running the following function string_tokens so that you can maintain a dataframe with a tokekenized text field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_tokens(corpus,column):\n",
    "    '''    \n",
    "    corpus: dataframe with tokenized text field\n",
    "    column: text column\n",
    "    \n",
    "    output: dataframe with string for text field\n",
    "    '''\n",
    "    \n",
    "    # returns tokinzed columns back to strings\n",
    "    for i , row in enumerate(corpus[column]):\n",
    "        corpus[column][i]=' '.join(corpus[column][i])\n",
    "    # returns clean strings df\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_clean_string_corpus(corpus,column):\n",
    "    \n",
    "    '''\n",
    "    runs eac of three functions and outputs a dataframe where \n",
    "    the text field has had punctuation and stop words removed removed &\n",
    "    haas been returned to a string to a string.\n",
    "   \n",
    "    '''\n",
    "    # runs each of the three prior functions and returns  \n",
    "    corpus = make_tokens(corpus,column)\n",
    "    corpus = clean_tokens(corpus, column)\n",
    "    corpus = string_tokens(corpus,column)\n",
    "            \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Pipelines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we has a series of functions created for the fitting and scoreing of numerous pieplines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example list of pipelines requiered to run these functions\n",
    "\n",
    "pipelines = [\n",
    "\n",
    "    (\"ModelName_1\", pipeline_1),\n",
    "    \n",
    "    (\"ModelName_2\", pipeline_2),\n",
    "\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pipes(pipelines,X,y):\n",
    "    \"\"\"\n",
    "     fits a model for each pipeline in our pipelines list\n",
    "    \"\"\"\n",
    "    for name, model in pipelines:\n",
    "        model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_pipes(pipelines,X,y):\n",
    "    \"\"\"\n",
    "    score all pipes in pipelines\n",
    "    pipelines: list of tuples\n",
    "    \n",
    "    X = features of population to score .. ie ( X_train )\n",
    "    y = target feature population to score .. ie ( y_train )\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # for each name and model in pipelines\n",
    "    for name, model in pipelines:\n",
    "        # score the given population \n",
    "        scores.append(model.score(X,y))\n",
    "        #print the madel name and score\n",
    "        print( f'{name} Accuracy: {model.score(X,y)} ' )\n",
    "    #retunrs all scores in a datframe.\n",
    "\n",
    "    df['Scores'] = scores\n",
    "    df['Model'] = [name for name, model in pipelines]\n",
    "    df.set_index('Model')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For model selection assesing the varriance in Train and test Scores is made easier by visualizing the gap between them among models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gap(pipelines,train_scores,test_scores):\n",
    "    ''' \n",
    "    for assesing the gap between train and test model scores\n",
    "    \n",
    "    pipelines: list fo tuples\n",
    "    scores: lists\n",
    "    \n",
    "    output: dataframe\n",
    "    '''\n",
    "    output = []\n",
    "    \n",
    "    for i , li in enumerate(train_scores):\n",
    "        dif =  li - test_scores[i]\n",
    "        output.append(dif)\n",
    "    result = pd.DataFrame()\n",
    "    result['Model'] = [name for name , model in pipelines]\n",
    "    result['Gap'] = [li for li in output]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def best_score(pipelines,X,y):\n",
    "    \"\"\"\n",
    "    returns the name and score of the model with the highest test score\n",
    "    \n",
    "    pipelines: list of tuples\n",
    "    \n",
    "    X = features of population to score .. ie ( X_train )\n",
    "    y = target feature population to score .. ie ( y_train )\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "    best_classifier = '' \n",
    "    best_pipeline = ''\n",
    "    \n",
    "    \n",
    "    \n",
    "    for name, model in pipelines:\n",
    "        if model.score(X,y) > best_accuracy:\n",
    "            best_accuracy = model.score(X, y)\n",
    "            best_pipeline = model\n",
    "            best_classifier = name\n",
    "            \n",
    "    print(f' Best Accuracy: {best_accuracy}')\n",
    "    print(f'Model: {best_classifier}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes it easy for us to concatonate the results of our 3 modle test scores metrics into one datframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_results_df(pipelines,X_train,y_train,X_test,y_test):\n",
    "    \n",
    "    '''\n",
    "    This function scores test and train population for all models in pipelines list\n",
    "    The Gap is calculated by taking the difference in scores\n",
    "    A dataframe is returned with train, test , and gap values for each model.\n",
    "    '''\n",
    "    \n",
    "    train_scores = score_pipes(pipelines,X_train,y_train)\n",
    "    test_scores = score_pipes(pipelines,X_test,y_test)\n",
    "    output = pd.merge(left = train_scores,right =  test_scores, left_on = 'Model',right_on = 'Model')\n",
    "    train_score_list = [ score for score in train_scores['Scores']]\n",
    "    test_score_list = [ score for score in test_scores['Scores']]\n",
    "    gap_results = get_gap(pipelines,train_score_list,test_score_list)\n",
    "    output = pd.merge(output, gap_results, left_on='Model', right_on='Model')\n",
    "    output.set_index('Model')\n",
    "    return output\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
